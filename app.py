import streamlit as st
from openai import OpenAI
from PIL import Image
import io
import base64
import re
import os
import json
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
import gspread
from google.oauth2.service_account import Credentials

# 0. ç¯å¢ƒé…ç½®
load_dotenv()
st.set_page_config(page_title="Health Dashboard Pro", layout="wide", page_icon="ğŸ«€")

# --- éšè—å¼é…ç½®è¯»å– ---
if "POIXE_API_KEY" in st.secrets:
    api_key = st.secrets["POIXE_API_KEY"]
    api_status = "âœ… API Key å·²é…ç½®"
else:
    api_key = os.getenv("POIXE_API_KEY", "")
    api_status = "âš ï¸ æœªæ£€æµ‹åˆ° Secrets API Key"

if "spreadsheet_url" in st.secrets:
    SHEET_URL = st.secrets["spreadsheet_url"]
    sheet_status = "âœ… Google Sheet è¿æ¥å°±ç»ª"
else:
    SHEET_URL = ""
    sheet_status = "âš ï¸ æœªé…ç½® Google Sheet URL"

# 1. æ ¸å¿ƒå·¥å…·å‡½æ•°
def smart_process_image(uploaded_file):
    uploaded_file.seek(0)
    file_bytes = uploaded_file.getvalue()
    if len(file_bytes) / 1024 < 500:
        return file_bytes, "image/jpeg"

    image = Image.open(uploaded_file)
    if image.mode in ("RGBA", "P"):
        image = image.convert("RGB")
        
    filename = uploaded_file.name
    is_screenshot = any(k in filename for k in ["Screenshot", "SHealth", "ReactNative", "å±å¹•æˆªå›¾"])
    
    buffer = io.BytesIO()
    if is_screenshot:
        image.save(buffer, format="JPEG", quality=95)
    else:
        target_width = 2048
        if image.width > target_width:
            ratio = target_width / image.width
            new_height = int(image.height * ratio)
            image = image.resize((target_width, new_height), Image.Resampling.LANCZOS)
        image.save(buffer, format="JPEG", quality=75)
    return buffer.getvalue(), "image/jpeg"

def parse_file_info(filename):
    """
    æ–‡ä»¶åè§£æé€»è¾‘
    """
    # 1. æ˜¾å¼å…³é”®å­—åŒ¹é…
    if "ReactNative" in filename or "Screenshot" in filename or "å±å¹•æˆªå›¾" in filename:
        return None, 'workout_snapshot'
    if "SHealth" in filename:
        return None, 's_health'
    
    # 2. çº¯æ•°å­—æ–‡ä»¶ååŒ¹é… (å¦‚ 1769760746481.jpg)
    if re.match(r'^\d{13}\.', filename):
        return None, 's_health'
    
    # 3. æ—¥æœŸåŒ¹é… (YYYYMMDD)
    match_full = re.search(r'(20\d{2})(\d{2})(\d{2})_(\d{6})', filename)
    if match_full:
        try:
            y, m, d, t = match_full.groups()
            dt_obj = datetime.strptime(f"{y}{m}{d}{t}", "%Y%m%d%H%M%S")
            return dt_obj, 'food'
        except:
            pass

    # 4. æ—¶é—´åŒ¹é… (Fallback)
    match_time = re.search(r'_(\d{6})', filename)
    if match_time:
        try:
            t_str = match_time.group(1)
            if int(t_str) < 240000:
                now = datetime.now()
                t_obj = datetime.strptime(t_str, "%H%M%S").time()
                return datetime.combine(now.date(), t_obj), 'food'
        except:
            pass
            
    return None, 'food'

def extract_json_from_response(text):
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    text = re.sub(r"```json\s*", "", text, flags=re.IGNORECASE)
    text = re.sub(r"```", "", text)
    match = re.search(r'(\{.*\})', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except:
            pass
    return {}

def normalize_data(data, target_date=None):
    if target_date:
        current_dt = target_date
    else:
        current_dt = datetime.now()
        
    week_map = {0:"ä¸€", 1:"äºŒ", 2:"ä¸‰", 3:"å››", 4:"äº”", 5:"å…­", 6:"æ—¥"}
    data['æ—¥æœŸ'] = current_dt.strftime("%Y-%m-%d")
    data['æ˜ŸæœŸ'] = f"å‘¨{week_map[current_dt.weekday()]}"
    
    default_schema = {
        "è¥å…»æ‘„å…¥æ±‡æ€»": {
            "æ€»çƒ­é‡": 0, "æ€»è›‹ç™½è´¨": 0, "æ€»ç¢³æ°´": 0, "æ€»è„‚è‚ª": 0, "æ€»è†³é£Ÿçº¤ç»´": 0, 
            "æ€»ç›ˆä½™ç¼ºå£åˆ†æ": "æš‚æ— åˆ†æ"
        },
        "æ—©é¤": {"æ—¶é—´": "N/A", "å†…å®¹": "", "çƒ­é‡": 0, "è›‹ç™½è´¨": 0, "ç¢³æ°´": 0, "è„‚è‚ª": 0, "è†³é£Ÿçº¤ç»´": 0, "ç‚¹è¯„": ""},
        "åˆé¤": {"æ—¶é—´": "N/A", "å†…å®¹": "", "çƒ­é‡": 0, "è›‹ç™½è´¨": 0, "ç¢³æ°´": 0, "è„‚è‚ª": 0, "è†³é£Ÿçº¤ç»´": 0, "ç‚¹è¯„": ""},
        "æ™šé¤": {"æ—¶é—´": "N/A", "å†…å®¹": "", "çƒ­é‡": 0, "è›‹ç™½è´¨": 0, "ç¢³æ°´": 0, "è„‚è‚ª": 0, "è†³é£Ÿçº¤ç»´": 0, "ç‚¹è¯„": ""},
        "åŠ é¤": {"æ—¶é—´": "N/A", "å†…å®¹": "", "çƒ­é‡": 0, "è›‹ç™½è´¨": 0, "ç¢³æ°´": 0, "è„‚è‚ª": 0, "è†³é£Ÿçº¤ç»´": 0, "ç‚¹è¯„": ""},
        "ç¡çœ ": {
            "å…¥ç¡æ—¶é—´": "N/A", "èµ·åºŠæ—¶é—´": "N/A", "ç¡çœ æ€»æ—¶é•¿": "0h", 
            "ç¡çœ é˜¶æ®µåˆ†æ": "æš‚æ— æ•°æ®", "ç¡çœ ç‚¹è¯„": ""
        },
        "å¿ƒç‡": {
            "é™æ¯å¿ƒç‡": 0, "å¹³å‡é™æ¯èŒƒå›´": "N/A", "å…¨å¤©å¿ƒç‡èŒƒå›´": "N/A", 
            "å¿ƒç‡æ—¶åºåˆ†æ": "æš‚æ— æ•°æ®", "å¿ƒç‡ç‚¹è¯„": ""
        },
        "å‹åŠ›": {
            "å‹åŠ›å‡å€¼": 0, "å‹åŠ›æ—¶åºåˆ†æ": "æš‚æ— æ•°æ®", "å‹åŠ›ç‚¹è¯„": "",
        },
        "å…¨å¤©æ¶ˆè€—ä¸æ´»åŠ¨": {
            "æ€»æ­¥æ•°": 0, "æ´»åŠ¨æ—¶é•¿": "0min", "æ´»åŠ¨å¡è·¯é‡Œ": 0, "ç‡ƒçƒ§çš„å¡è·¯é‡Œæ€»æ•°": 0
        },
        "åŠ›é‡è®­ç»ƒ": {
            "åŠ›é‡ä¸»é¢˜": "ä¼‘æ¯æ—¥", "å…·ä½“æ—¶é—´": "N/A", "è®­ç»ƒæ—¶é•¿": "0min", 
            "æ€»å®¹é‡": 0, "æ¶ˆè€—ä¼°ç®—": 0, "åŠ›é‡ç‚¹è¯„": "",
            "åŠ¨ä½œæµæ°´æ˜ç»†": []
        },
        "æœ‰æ°§è®­ç»ƒ": {
            "æœ‰æ°§ç±»å‹": "æ— ", "å…·ä½“æ—¶é—´": "N/A", "è·ç¦»": "0km", "æœ‰æ°§æ—¶é•¿": "0min", 
            "å¹³å‡å¿ƒç‡": 0, "å¹³å‡æ­¥é¢‘": 0, "å¹³å‡æ­¥é€Ÿ": "N/A", "æœ‰æ°§å¡è·¯é‡Œæ¶ˆè€—": 0
        },
        "æœ¬æ—¥æ€»ç»“": {"æœ¬æ—¥åˆ†æ": "", "æŒ‡å¯¼å»ºè®®": ""}
    }

    for k, v in default_schema.items():
        if k not in data:
            data[k] = v
        elif isinstance(v, dict):
            for sub_k, sub_v in v.items():
                if sub_k not in data[k]:
                    data[k][sub_k] = sub_v
                    
    return data

def save_data_to_gsheet(data, sheet_url):
    row = []
    row.append(data.get('æ—¥æœŸ'))
    row.append(data.get('æ˜ŸæœŸ'))
    
    summ = data.get('è¥å…»æ‘„å…¥æ±‡æ€»', {})
    row.extend([
        summ.get('æ€»çƒ­é‡'), summ.get('æ€»è›‹ç™½è´¨'), summ.get('æ€»ç¢³æ°´'), 
        summ.get('æ€»è„‚è‚ª'), summ.get('æ€»è†³é£Ÿçº¤ç»´'), summ.get('æ€»ç›ˆä½™ç¼ºå£åˆ†æ')
    ])
    
    meals = ['æ—©é¤', 'åˆé¤', 'æ™šé¤', 'åŠ é¤']
    for m in meals:
        meal = data.get(m, {})
        row.extend([
            meal.get('æ—¶é—´'), meal.get('å†…å®¹'), meal.get('çƒ­é‡'),
            meal.get('è›‹ç™½è´¨'), meal.get('ç¢³æ°´'), meal.get('è„‚è‚ª'),
            meal.get('è†³é£Ÿçº¤ç»´'), meal.get('ç‚¹è¯„')
        ])
        
    slp = data.get('ç¡çœ ', {})
    row.extend([
        slp.get('å…¥ç¡æ—¶é—´'), slp.get('èµ·åºŠæ—¶é—´'), slp.get('ç¡çœ æ€»æ—¶é•¿'),
        slp.get('ç¡çœ é˜¶æ®µåˆ†æ'), slp.get('ç¡çœ ç‚¹è¯„')
    ])
    
    hr = data.get('å¿ƒç‡', {})
    row.extend([
        hr.get('é™æ¯å¿ƒç‡'), hr.get('å¹³å‡é™æ¯èŒƒå›´'), hr.get('å…¨å¤©å¿ƒç‡èŒƒå›´'),
        hr.get('å¿ƒç‡æ—¶åºåˆ†æ'), hr.get('å¿ƒç‡ç‚¹è¯„')
    ])
    
    stres = data.get('å‹åŠ›', {})
    row.extend([
        stres.get('å‹åŠ›å‡å€¼'), stres.get('å‹åŠ›æ—¶åºåˆ†æ'), stres.get('å‹åŠ›ç‚¹è¯„')
    ])
    
    act = data.get('å…¨å¤©æ¶ˆè€—ä¸æ´»åŠ¨', {})
    row.extend([
        act.get('æ€»æ­¥æ•°'), act.get('æ´»åŠ¨æ—¶é•¿'), act.get('æ´»åŠ¨å¡è·¯é‡Œ'), act.get('ç‡ƒçƒ§çš„å¡è·¯é‡Œæ€»æ•°')
    ])
    
    stren = data.get('åŠ›é‡è®­ç»ƒ', {})
    details = stren.get('åŠ¨ä½œæµæ°´æ˜ç»†', [])
    details_str = ""
    if isinstance(details, list):
        details_list = [f"{d.get('åŠ¨ä½œåç§°','')}({d.get('é‡é‡','')}kg*{d.get('æ¬¡æ•°','')})" for d in details]
        details_str = " | ".join(details_list)
        
    row.extend([
        stren.get('åŠ›é‡ä¸»é¢˜'), stren.get('å…·ä½“æ—¶é—´'), stren.get('è®­ç»ƒæ—¶é•¿'),
        details_str, 
        stren.get('æ€»å®¹é‡'), stren.get('æ¶ˆè€—ä¼°ç®—'), stren.get('åŠ›é‡ç‚¹è¯„')
    ])
    
    cardio = data.get('æœ‰æ°§è®­ç»ƒ', {})
    row.extend([
        cardio.get('æœ‰æ°§ç±»å‹'), cardio.get('å…·ä½“æ—¶é—´'), cardio.get('è·ç¦»'),
        cardio.get('æœ‰æ°§æ—¶é•¿'), cardio.get('å¹³å‡å¿ƒç‡'), cardio.get('å¹³å‡æ­¥é¢‘'),
        cardio.get('å¹³å‡æ­¥é€Ÿ'), cardio.get('æœ‰æ°§å¡è·¯é‡Œæ¶ˆè€—')
    ])
    
    summ_txt = data.get('æœ¬æ—¥æ€»ç»“', {})
    row.extend([
        summ_txt.get('æœ¬æ—¥åˆ†æ'), summ_txt.get('æŒ‡å¯¼å»ºè®®')
    ])

    try:
        if "gcp_service_account" in st.secrets:
            creds_dict = dict(st.secrets["gcp_service_account"])
            creds_dict["private_key"] = creds_dict["private_key"].replace("\\n", "\n")
            
            scopes = [
                "https://www.googleapis.com/auth/spreadsheets",
                "https://www.googleapis.com/auth/drive"
            ]
            creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
            client = gspread.authorize(creds)
            
            try:
                sheet = client.open_by_url(sheet_url).sheet1
            except gspread.SpreadsheetNotFound:
                return False, "æ‰¾ä¸åˆ°è¡¨æ ¼ï¼Œè¯·æ£€æŸ¥ URL æˆ–æƒé™"
                
            sheet.append_row(row)
            return True, "å†™å…¥æˆåŠŸ"
        else:
            return False, "æœªé…ç½® Google Service Account å‡­è¯"
    except Exception as e:
        return False, str(e)


# 2. Payload æ„å»º
def build_payload(uploaded_files, quick_adds):
    timeline_fixed = []   
    timeline_float = []   
    valid_dates = [] 
    
    with st.status("æ­£åœ¨å¤„ç†å›¾åƒ...", expanded=False) as status:
        for file in uploaded_files:
            b64_str, mime = smart_process_image(file)
            b64_encoded = base64.b64encode(b64_str).decode('utf-8')
            
            file_dt, file_type = parse_file_info(file.name)
            
            item = {"type": "image", "name": file.name, "data": b64_encoded, "mime": mime, "file_type": file_type}
            
            if file_type == 'food':
                if file_dt:
                    item['time'] = file_dt
                    timeline_fixed.append(item)
                    if file_dt.year > 2000:
                        valid_dates.append(file_dt)
                else:
                    item['label'] = "ã€æœªå½’æ¡£é£Ÿç‰©ã€‘"
                    timeline_float.append(item)
            elif file_type == 'workout_snapshot':
                item['label'] = "ã€å¥èº«è¯¦æƒ…æˆªå›¾ã€‘"
                timeline_float.append(item)
                if file_dt and file_dt.year > 2000:
                    valid_dates.append(file_dt)
            elif file_type == 's_health':
                item['label'] = "ã€SHealthæ±‡æ€»ã€‘"
                timeline_float.append(item)
                    
        status.update(label="å›¾åƒå¤„ç†å®Œæˆ", state="complete")

    if valid_dates:
        report_date = min(valid_dates)
    else:
        report_date = datetime.now()

    timeline_fixed.sort(key=lambda x: x['time'])

    user_content = []
    user_content.append({"type": "text", "text": "## Part 1: é¥®é£Ÿç…§ç‰‡æµ\n(è¯·å¯¹ä»¥ä¸‹é£Ÿç‰©ç…§ç‰‡è¿›è¡Œç²¾ç¡®è§†è§‰ä¼°ç®—ï¼ŒåŒ…å«çƒ­é‡, è›‹ç™½è´¨, ç¢³æ°´, è„‚è‚ª, è†³é£Ÿçº¤ç»´)\n"})
    for item in timeline_fixed:
        t = item['time'].strftime("%H:%M")
        if item.get('type') == 'text':
            user_content.append({"type": "text", "text": f"- {t} {item.get('content')}"})
        else:
            user_content.append({"type": "text", "text": f"- {t} [é£Ÿç‰©ç…§ç‰‡] (è¯·ä¼°ç®—çƒ­é‡åŠå®é‡è¥å…»ç´ )"})
            user_content.append({"type": "image_url", "image_url": {"url": f"data:{item['mime']};base64,{item['data']}"}})

    supplement_text = ""
    if quick_adds.get('bcaa'): supplement_text += "- BCAA 6g (è®­ç»ƒä¸­æ‘„å…¥)\n"
    if quick_adds.get('protein'): supplement_text += "- è›‹ç™½ç²‰ 32g + è‚Œé…¸ 3g (è®­ç»ƒåæ‘„å…¥)\n"
    if supplement_text:
        user_content.append({
            "type": "text", 
            "text": f"\n## ç‰¹åˆ«æŒ‡ä»¤ï¼šè¡¥å‰‚\nã€å¼ºåˆ¶è¦æ±‚ã€‘è¯·å°†ä»¥ä¸‹è¡¥å‰‚åˆå¹¶è®¡ç®—å…¥ JSON çš„ `åŠ é¤` å­—æ®µï¼š\n{supplement_text}"
        })

    imgs = [x for x in timeline_float if x['file_type'] in ['workout_snapshot', 's_health']]
    if imgs:
        user_content.append({"type": "text", "text": "\n## Part 2: å¥åº·æ•°æ®æˆªå›¾ (OCR)\nè¯·æå–åŒ…æ‹¬æ­¥é¢‘ã€é…é€Ÿã€å‹åŠ›æ—¶åºç­‰æ‰€æœ‰è¯¦ç»†æ•°æ®ã€‚\n"})
        for img in imgs:
            user_content.append({"type": "text", "text": f"ğŸ“¸ {img['label']}"})
            user_content.append({"type": "image_url", "image_url": {"url": f"data:{img['mime']};base64,{img['data']}"}})
            
    return user_content, report_date

# 3. JSON Schema
RESPONSE_SCHEMA = """
{
  "è¥å…»æ‘„å…¥æ±‡æ€»": {
    "æ€»çƒ­é‡": 0, "æ€»è›‹ç™½è´¨": 0, "æ€»ç¢³æ°´": 0, "æ€»è„‚è‚ª": 0, "æ€»è†³é£Ÿçº¤ç»´": 0,
    "æ€»ç›ˆä½™ç¼ºå£åˆ†æ": "..."
  },
  "æ—©é¤": { 
    "æ—¶é—´": "HH:MM", "å†…å®¹": "...", "çƒ­é‡": 0, "è›‹ç™½è´¨": 0, "ç¢³æ°´": 0, "è„‚è‚ª": 0, "è†³é£Ÿçº¤ç»´": 0, "ç‚¹è¯„": "..." 
  },
  "åˆé¤": { 
    "æ—¶é—´": "HH:MM", "å†…å®¹": "...", "çƒ­é‡": 0, "è›‹ç™½è´¨": 0, "ç¢³æ°´": 0, "è„‚è‚ª": 0, "è†³é£Ÿçº¤ç»´": 0, "ç‚¹è¯„": "..." 
  },
  "æ™šé¤": { 
    "æ—¶é—´": "HH:MM", "å†…å®¹": "...", "çƒ­é‡": 0, "è›‹ç™½è´¨": 0, "ç¢³æ°´": 0, "è„‚è‚ª": 0, "è†³é£Ÿçº¤ç»´": 0, "ç‚¹è¯„": "..." 
  },
  "åŠ é¤": { 
    "æ—¶é—´": "HH:MM", "å†…å®¹": "...", "çƒ­é‡": 0, "è›‹ç™½è´¨": 0, "ç¢³æ°´": 0, "è„‚è‚ª": 0, "è†³é£Ÿçº¤ç»´": 0, "ç‚¹è¯„": "..." 
  },
  "ç¡çœ ": { 
    "å…¥ç¡æ—¶é—´": "HH:MM", "èµ·åºŠæ—¶é—´": "HH:MM", "ç¡çœ æ€»æ—¶é•¿": "...", 
    "ç¡çœ é˜¶æ®µåˆ†æ": "...", "ç¡çœ ç‚¹è¯„": "..." 
  },
  "å¿ƒç‡": { 
    "é™æ¯å¿ƒç‡": 0, "å¹³å‡é™æ¯èŒƒå›´": "...", "å…¨å¤©å¿ƒç‡èŒƒå›´": "...", 
    "å¿ƒç‡æ—¶åºåˆ†æ": "...", "å¿ƒç‡ç‚¹è¯„": "..." 
  },
  "å‹åŠ›": { 
    "å‹åŠ›å‡å€¼": 0, "å‹åŠ›æ—¶åºåˆ†æ": "...", "å‹åŠ›ç‚¹è¯„": "..." 
  },
  "å…¨å¤©æ¶ˆè€—ä¸æ´»åŠ¨": { 
    "æ€»æ­¥æ•°": 0, "æ´»åŠ¨æ—¶é•¿": "...", "æ´»åŠ¨å¡è·¯é‡Œ": 0, "ç‡ƒçƒ§çš„å¡è·¯é‡Œæ€»æ•°": 0 
  },
  "åŠ›é‡è®­ç»ƒ": {
    "åŠ›é‡ä¸»é¢˜": "...", "å…·ä½“æ—¶é—´": "HH:MM", "è®­ç»ƒæ—¶é•¿": "...",
    "åŠ¨ä½œæµæ°´æ˜ç»†": [ 
      { 
        "åŠ¨ä½œåç§°": "...", "OCRåŸå§‹è¡Œ": "å¦‚: 1/çƒ­ 10+10kg 12", "ç»„åºå·": "1", "é‡é‡": 20, "æ¬¡æ•°": 12 
      } 
    ],
    "æ€»å®¹é‡": 0, "æ¶ˆè€—ä¼°ç®—": 0, "åŠ›é‡ç‚¹è¯„": "..."
  },
  "æœ‰æ°§è®­ç»ƒ": { 
    "æœ‰æ°§ç±»å‹": "...", "å…·ä½“æ—¶é—´": "HH:MM", "è·ç¦»": "...", "æœ‰æ°§æ—¶é•¿": "...", 
    "å¹³å‡å¿ƒç‡": "...", "å¹³å‡æ­¥é¢‘": "...", "å¹³å‡æ­¥é€Ÿ": "...", "æœ‰æ°§å¡è·¯é‡Œæ¶ˆè€—": "..." 
  },
  "æœ¬æ—¥æ€»ç»“": { "æœ¬æ—¥åˆ†æ": "...", "æŒ‡å¯¼å»ºè®®": "..." }
}
"""

# 4. UI ä¸»ç¨‹åº

with st.sidebar:
    st.markdown("âš™ï¸ **ç³»ç»ŸçŠ¶æ€**")
    st.caption(f"API Connection: {api_status}")
    st.caption(f"Storage: {sheet_status}")
    
    st.divider()
    st.markdown("ğŸ’¾ **è®¾ç½®**")
    auto_save = st.checkbox("è‡ªåŠ¨åŒæ­¥åˆ° Google Sheets", value=True, disabled=(SHEET_URL==""))

uploaded_files = st.file_uploader("ğŸ“¤ **ä¸Šä¼ è®°å½• (æˆªå›¾/é£Ÿç‰©)**", accept_multiple_files=True, type=['jpg', 'jpeg', 'png'])

# å¿«é€Ÿè¡¥å‰‚ç§»è‡³ä¸»é¡µé¢
qc1, qc2 = st.columns(2)
with qc1:
    opt_bcaa = st.checkbox(''':blue-background[ğŸ¥¤ ç»ƒä¸­ BCAA]''')
with qc2:
    opt_protein = st.checkbox(''':blue-background[ğŸ¥› ç»ƒå è›‹ç™½ç²‰]''')
quick_adds = {"bcaa": opt_bcaa, "protein": opt_protein}

if st.button("ğŸš€ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š", type="primary"):
    if not uploaded_files:
        st.warning("è¯·ä¸Šä¼ å›¾ç‰‡")
        st.stop()
    if not api_key:
        st.error("æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·æ£€æŸ¥ secrets.toml é…ç½®")
        st.stop()
        
    try:
        user_content, report_date = build_payload(uploaded_files, quick_adds)
        
        system_prompt = f"""ä½ æ˜¯ä¸€åç²¾è‹±è¥å…»å¸ˆå’Œæ•°æ®åˆ†æå¸ˆã€‚
        
        ã€ä»»åŠ¡ 1ï¼šåŠ›é‡è®­ç»ƒ - é€è¡Œæå–ã€‘
        **ä¸è¦åˆå¹¶ï¼** æˆªå›¾æœ‰å‡ ç»„ï¼Œæ•°ç»„é‡Œå°±æœ‰å‡ ä¸ªå¯¹è±¡ã€‚
        **ä¸è¦ä¹˜åºå·ï¼** å•ç»„å®¹é‡ = é‡é‡ * æ¬¡æ•°ã€‚
        
        ã€ä»»åŠ¡ 2ï¼šè†³é£Ÿçº¤ç»´ä¸è¥å…»ã€‘
        å¯¹é£Ÿç‰©ç…§ç‰‡è¿›è¡Œä¼°ç®—æ—¶ï¼Œå¿…é¡»è¿›è¡Œç²¾ç¡®è§†è§‰ä¼°ç®—ï¼ŒåŒ…å«çƒ­é‡, è›‹ç™½è´¨, ç¢³æ°´, è„‚è‚ª, è†³é£Ÿçº¤ç»´æ•°æ®ã€‚
        
        ã€ä»»åŠ¡ 3ï¼šå‹åŠ›å‡å€¼ã€‘
        è‹¥æ— ç›´æ¥å‡å€¼ï¼ŒæŒ‰ (é«˜*90 + ä¸­*65 + ä½*40 + æ”¾æ¾*10)/100 è®¡ç®—ã€‚

        ã€è¾“å‡ºè¦æ±‚ã€‘
        ä¸¥æ ¼ JSON æ ¼å¼ï¼Œä¸è¦å¤šä½™æ–‡æœ¬ã€‚
        {RESPONSE_SCHEMA}
        """
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content}
        ]

        client = OpenAI(api_key=api_key, base_url="https://api.poixe.com/v1")
        
        with st.spinner("æ­£åœ¨å…¨ç»´åº¦è§£æ..."):
            response = client.chat.completions.create(
                model="gemini-2.0-flash", 
                messages=messages,
                temperature=0.0, 
                response_format={"type": "json_object"}
            )
            
        result_text = response.choices[0].message.content
        raw_data = extract_json_from_response(result_text)
        
        # === æ•°æ®å½’ä¸€åŒ– ===
        data = normalize_data(raw_data, target_date=report_date)
        
        # === åŠ›é‡æ•°æ®èšåˆ ===
        workout_df = pd.DataFrame()
        strength_data = data.get('åŠ›é‡è®­ç»ƒ', {})
        details = strength_data.get('åŠ¨ä½œæµæ°´æ˜ç»†', [])
        
        total_vol = 0
        if details:
            for d in details:
                try:
                    w = float(d.get('é‡é‡', 0))
                    r = float(d.get('æ¬¡æ•°', 0))
                    d['å•ç»„å®¹é‡'] = w * r
                    total_vol += d['å•ç»„å®¹é‡']
                except:
                    d['å•ç»„å®¹é‡'] = 0
            strength_data['æ€»å®¹é‡'] = total_vol 
            workout_df = pd.DataFrame(details)
        
        # === çŠ¶æ€åŒæ­¥ ===
        st.toast(f"âœ… è§£æå®Œæˆ | æ—¥æœŸ: {data['æ—¥æœŸ']}", icon="ğŸ“…")
        
        if auto_save and SHEET_URL:
            with st.spinner("æ­£åœ¨åŒæ­¥åˆ°äº‘ç«¯..."):
                success, msg = save_data_to_gsheet(data, SHEET_URL)
                if success:
                    st.toast("âœ… æ•°æ®å·²åŒæ­¥åˆ° Google Sheet", icon="â˜ï¸")
                else:
                    st.error(f"âŒ åŒæ­¥å¤±è´¥: {msg}")
        
        # ==========================================
        # 5. ä¸“ä¸šè¡¨æ ¼åŒ–å±•ç¤º (Mobile Optimized - Direct Display)
        # ==========================================
        
        # --- æ ¸å¿ƒæ‘˜è¦è¡¨ ---
        st.markdown("ğŸ“Š **æ¯æ—¥æ¦‚è§ˆ**")
        summary_data = [
            {"æŒ‡æ ‡": "æ€»æ‘„å…¥", "æ•°å€¼": f"{data['è¥å…»æ‘„å…¥æ±‡æ€»']['æ€»çƒ­é‡']} kcal", "è¯¦æƒ…": f"Fib: {data['è¥å…»æ‘„å…¥æ±‡æ€»']['æ€»è†³é£Ÿçº¤ç»´']}g, Pro: {data['è¥å…»æ‘„å…¥æ±‡æ€»']['æ€»è›‹ç™½è´¨']}g"},
            {"æŒ‡æ ‡": "æ€»æ¶ˆè€—", "æ•°å€¼": f"{data['å…¨å¤©æ¶ˆè€—ä¸æ´»åŠ¨']['ç‡ƒçƒ§çš„å¡è·¯é‡Œæ€»æ•°']} kcal", "è¯¦æƒ…": "åŒ…å«åŸºç¡€ä»£è°¢ä¸æ´»åŠ¨æ¶ˆè€—"},
            {"æŒ‡æ ‡": "çƒ­é‡å·®", "æ•°å€¼": f"{data['è¥å…»æ‘„å…¥æ±‡æ€»']['æ€»çƒ­é‡'] - data['å…¨å¤©æ¶ˆè€—ä¸æ´»åŠ¨']['ç‡ƒçƒ§çš„å¡è·¯é‡Œæ€»æ•°']} kcal", "è¯¦æƒ…": data['è¥å…»æ‘„å…¥æ±‡æ€»']['æ€»ç›ˆä½™ç¼ºå£åˆ†æ']},
            {"æŒ‡æ ‡": "è®­ç»ƒå®¹é‡", "æ•°å€¼": f"{int(total_vol)} kg", "è¯¦æƒ…": strength_data.get('åŠ›é‡ä¸»é¢˜', 'ä¼‘æ¯æ—¥')},
            {"æŒ‡æ ‡": "å‹åŠ›å‡å€¼", "æ•°å€¼": f"{data['å‹åŠ›']['å‹åŠ›å‡å€¼']}", "è¯¦æƒ…": data['å‹åŠ›']['å‹åŠ›ç‚¹è¯„'][:20]+"..."}
        ]
        st.dataframe(pd.DataFrame(summary_data), width="stretch", hide_index=True)

        st.divider()

        # --- 1. é¥®é£Ÿè¯¦æƒ… ---
        st.markdown("ğŸ½ï¸ **é¥®é£Ÿè¯¦æƒ…**")
        macros_data = []
        for m in ['æ—©é¤', 'åˆé¤', 'æ™šé¤', 'åŠ é¤']:
            row = data[m]
            macros_data.append({
                "é¤åˆ«": m,
                "æ—¶é—´": row['æ—¶é—´'],
                "å†…å®¹": row['å†…å®¹'],
                "Cal": row['çƒ­é‡'],
                "P": row['è›‹ç™½è´¨'],
                "C": row['ç¢³æ°´'],
                "F": row['è„‚è‚ª'],
                "Fib": row['è†³é£Ÿçº¤ç»´']
            })
        df_macros = pd.DataFrame(macros_data)
        st.dataframe(df_macros, width="stretch", hide_index=True)
        st.caption("æ³¨: P=è›‹ç™½è´¨, C=ç¢³æ°´, F=è„‚è‚ª, Fib=è†³é£Ÿçº¤ç»´ (å•ä½:g)")

        st.divider()

        # --- 2. åŠ›é‡è®­ç»ƒ ---
        st.markdown("ğŸ‹ï¸ **åŠ›é‡è®­ç»ƒ**")
        st.markdown(f"**ä¸»é¢˜: {strength_data.get('åŠ›é‡ä¸»é¢˜', 'æ— ')}**")
        wo_meta = [
            {"é¡¹ç›®": "å¼€å§‹æ—¶é—´", "æ•°æ®": strength_data.get('å…·ä½“æ—¶é—´')},
            {"é¡¹ç›®": "è®­ç»ƒæ—¶é•¿", "æ•°æ®": strength_data.get('è®­ç»ƒæ—¶é•¿')},
            {"é¡¹ç›®": "æ€»å®¹é‡", "æ•°æ®": f"{total_vol} kg"},
            {"é¡¹ç›®": "ä¼°ç®—æ¶ˆè€—", "æ•°æ®": f"{strength_data.get('æ¶ˆè€—ä¼°ç®—')} kcal"}
        ]
        st.dataframe(pd.DataFrame(wo_meta), width="stretch", hide_index=True)
        
        if not workout_df.empty and "åŠ¨ä½œåç§°" in workout_df.columns:
            workout_df['ç»„è¯¦æƒ…'] = workout_df.apply(
                lambda x: f"{x.get('é‡é‡',0)}kgÃ—{x.get('æ¬¡æ•°',0)}", axis=1
            )
            df_agg = workout_df.groupby("åŠ¨ä½œåç§°", as_index=False).agg({
                "ç»„è¯¦æƒ…": lambda x: " | ".join(x),
                "å•ç»„å®¹é‡": "sum",
                "OCRåŸå§‹è¡Œ": "count"
            })
            df_agg.columns = ["åŠ¨ä½œåç§°", "è®°å½•", "æ€»å®¹é‡", "ç»„æ•°"]
            df_agg = df_agg[["åŠ¨ä½œåç§°", "è®°å½•"]] 
            st.dataframe(df_agg, width="stretch", hide_index=True)
        
        st.info(f"ğŸ’¡ {strength_data.get('åŠ›é‡ç‚¹è¯„')}")

        st.divider()

        # --- 3. æœ‰æ°§è®­ç»ƒ ---
        st.markdown("ğŸƒ **æœ‰æ°§è®­ç»ƒ**")
        st.markdown(f"**é¡¹ç›®: {data['æœ‰æ°§è®­ç»ƒ']['æœ‰æ°§ç±»å‹']}**")
        ac = data['æœ‰æ°§è®­ç»ƒ']
        cardio_table = [
            {"æŒ‡æ ‡": "è·ç¦»", "æ•°å€¼": ac['è·ç¦»']},
            {"æŒ‡æ ‡": "æ—¶é•¿", "æ•°å€¼": ac['æœ‰æ°§æ—¶é•¿']},
            {"æŒ‡æ ‡": "é…é€Ÿ", "æ•°å€¼": ac['å¹³å‡æ­¥é€Ÿ']},
            {"æŒ‡æ ‡": "å¹³å‡å¿ƒç‡", "æ•°å€¼": f"{ac['å¹³å‡å¿ƒç‡']} bpm"},
            {"æŒ‡æ ‡": "æ¶ˆè€—", "æ•°å€¼": f"{ac['æœ‰æ°§å¡è·¯é‡Œæ¶ˆè€—']} kcal"}
        ]
        st.dataframe(pd.DataFrame(cardio_table), width="stretch", hide_index=True)

        st.divider()

        # --- 4. ç¡çœ ä¸å‹åŠ› ---
        st.markdown("ğŸ’¤ **ç¡çœ  & å‹åŠ›**")
        slp = data['ç¡çœ ']
        sts = data['å‹åŠ›']
        health_table = [
            {"ç±»åˆ«": "ç¡çœ ", "æŒ‡æ ‡": "æ—¶é—´", "æ•°å€¼": f"{slp['å…¥ç¡æ—¶é—´']} - {slp['èµ·åºŠæ—¶é—´']}"},
            {"ç±»åˆ«": "ç¡çœ ", "æŒ‡æ ‡": "æ—¶é•¿", "æ•°å€¼": slp['ç¡çœ æ€»æ—¶é•¿']},
            {"ç±»åˆ«": "å‹åŠ›", "æŒ‡æ ‡": "å‡å€¼", "æ•°å€¼": sts['å‹åŠ›å‡å€¼']},
            {"ç±»åˆ«": "å‹åŠ›", "æŒ‡æ ‡": "è¯„ä»·", "æ•°å€¼": sts['å‹åŠ›ç‚¹è¯„']}
        ]
        st.dataframe(pd.DataFrame(health_table), width="stretch", hide_index=True)
        st.caption(f"ç¡çœ åˆ†æ: {slp['ç¡çœ é˜¶æ®µåˆ†æ']}")

        st.divider()

        # --- 5. å¿ƒç‡ä¸æ´»åŠ¨ ---
        st.markdown("â¤ï¸ **å¿ƒç‡ & æ´»åŠ¨**")
        hr = data['å¿ƒç‡']
        act = data['å…¨å¤©æ¶ˆè€—ä¸æ´»åŠ¨']
        body_table = [
            {"ç±»åˆ«": "å¿ƒç‡", "æŒ‡æ ‡": "é™æ¯å¿ƒç‡", "æ•°å€¼": f"{hr['é™æ¯å¿ƒç‡']} bpm"},
            {"ç±»åˆ«": "å¿ƒç‡", "æŒ‡æ ‡": "å…¨å¤©èŒƒå›´", "æ•°å€¼": hr['å…¨å¤©å¿ƒç‡èŒƒå›´']},
            {"ç±»åˆ«": "æ´»åŠ¨", "æŒ‡æ ‡": "æ€»æ­¥æ•°", "æ•°å€¼": act['æ€»æ­¥æ•°']},
            {"ç±»åˆ«": "æ´»åŠ¨", "æŒ‡æ ‡": "æ´»åŠ¨çƒ­é‡", "æ•°å€¼": f"{act['æ´»åŠ¨å¡è·¯é‡Œ']} kcal"}
        ]
        st.dataframe(pd.DataFrame(body_table), width="stretch", hide_index=True)

        st.divider()

        # --- 6. æ€»ç»“ä¸å»ºè®® ---
        st.markdown("ğŸ“ **æ€»ç»“ä¸å»ºè®®**")
        st.markdown("ğŸ“… **æœ¬æ—¥åˆ†æ**")
        st.write(data['æœ¬æ—¥æ€»ç»“']['æœ¬æ—¥åˆ†æ'])
        st.markdown("ğŸ›¡ï¸ **æŒ‡å¯¼å»ºè®®**")
        st.success(data['æœ¬æ—¥æ€»ç»“']['æŒ‡å¯¼å»ºè®®'])
        
        with st.expander("æŸ¥çœ‹åŸå§‹ JSON"):
            st.json(data)
            
    except Exception as e:
        st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")